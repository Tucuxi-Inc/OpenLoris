# Environment Configuration
ENVIRONMENT=development
DEBUG=true

# Security
SECRET_KEY=your-secret-key-here-change-in-production

# Database (use these ports for Docker)
DATABASE_URL=postgresql://loris:password@localhost:5435/loris

# Redis Cache
REDIS_URL=redis://localhost:6385

# ============================================
# AI Provider Configuration
# ============================================
# Choose your AI provider: local_ollama, cloud_anthropic, cloud_bedrock, cloud_azure
AI_PROVIDER=local_ollama

# Option 1: Ollama (No API key needed, encrypted traffic, no prompt/output logging)
# Install from https://ollama.ai and pull the required models:
#   ollama pull nomic-embed-text    (required - embedding model for automation matching)
#   ollama pull qwen3-vl:235b-cloud (default inference model - runs on Ollama cloud)
#   ollama pull gpt-oss:120b-cloud  (fallback inference model)
# Cloud models run on Ollama infrastructure but traffic is encrypted and
# prompts/outputs are not stored. Local models also work (e.g. llama3.2).
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=qwen3-vl:235b-cloud
OLLAMA_FALLBACK_MODEL=gpt-oss:120b-cloud

# Option 2: Anthropic Claude (Cloud - Highest quality)
# Get API key from https://console.anthropic.com
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Option 3: AWS Bedrock (Enterprise - data stays in your AWS account)
# AWS_REGION=us-east-1
# AWS_BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# Option 4: Azure OpenAI (Enterprise - data stays in your Azure tenant)
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_KEY=your-azure-key
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# ============================================

# CORS Settings (comma-separated)
ALLOWED_ORIGINS=http://localhost:3005,http://localhost:8005

# File Upload Settings
MAX_UPLOAD_SIZE=10485760
UPLOAD_DIR=uploads

# Rate Limiting
RATE_LIMIT_REQUESTS=100

# Pagination
DEFAULT_PAGE_SIZE=20
MAX_PAGE_SIZE=100
